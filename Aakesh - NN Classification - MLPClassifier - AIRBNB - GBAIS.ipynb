{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial - Neural Network - Classification\n",
    "\n",
    "\n",
    "We will predict the price category, among 4 categories, of an AIRBNB listing (`price_category` column). This is a multi-class classification task.\n",
    "\n",
    "**The unit of analysis is an AIRBNB LISTING**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>host_is_superhost</th>\n",
       "      <th>host_identity_verified</th>\n",
       "      <th>neighbourhood_cleansed</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>property_type</th>\n",
       "      <th>room_type</th>\n",
       "      <th>accommodates</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>...</th>\n",
       "      <th>guests_included</th>\n",
       "      <th>price_per_extra_person</th>\n",
       "      <th>minimum_nights</th>\n",
       "      <th>number_of_reviews</th>\n",
       "      <th>number_days_btw_first_last_review</th>\n",
       "      <th>review_scores_rating</th>\n",
       "      <th>cancellation_policy</th>\n",
       "      <th>price</th>\n",
       "      <th>price_gte_150</th>\n",
       "      <th>price_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Roslindale</td>\n",
       "      <td>42.282619</td>\n",
       "      <td>-71.133068</td>\n",
       "      <td>House</td>\n",
       "      <td>Entire home/apt</td>\n",
       "      <td>4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>2.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>moderate</td>\n",
       "      <td>250</td>\n",
       "      <td>1</td>\n",
       "      <td>gte_226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>Roslindale</td>\n",
       "      <td>42.286241</td>\n",
       "      <td>-71.134374</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>Private room</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>36</td>\n",
       "      <td>804</td>\n",
       "      <td>94.0</td>\n",
       "      <td>moderate</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>lte_$75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Roslindale</td>\n",
       "      <td>42.292438</td>\n",
       "      <td>-71.135765</td>\n",
       "      <td>Apartment</td>\n",
       "      <td>Private room</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "      <td>3</td>\n",
       "      <td>41</td>\n",
       "      <td>2574</td>\n",
       "      <td>98.0</td>\n",
       "      <td>moderate</td>\n",
       "      <td>65</td>\n",
       "      <td>0</td>\n",
       "      <td>lte_$75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Roslindale</td>\n",
       "      <td>42.281106</td>\n",
       "      <td>-71.121021</td>\n",
       "      <td>House</td>\n",
       "      <td>Private room</td>\n",
       "      <td>4</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>moderate</td>\n",
       "      <td>75</td>\n",
       "      <td>0</td>\n",
       "      <td>lte_$75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Roslindale</td>\n",
       "      <td>42.284512</td>\n",
       "      <td>-71.136258</td>\n",
       "      <td>House</td>\n",
       "      <td>Private room</td>\n",
       "      <td>2</td>\n",
       "      <td>1.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>29</td>\n",
       "      <td>380</td>\n",
       "      <td>99.0</td>\n",
       "      <td>flexible</td>\n",
       "      <td>79</td>\n",
       "      <td>0</td>\n",
       "      <td>btw_$75-$150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   host_is_superhost  host_identity_verified neighbourhood_cleansed  \\\n",
       "0                  0                       0             Roslindale   \n",
       "1                  0                       1             Roslindale   \n",
       "2                  1                       1             Roslindale   \n",
       "3                  0                       0             Roslindale   \n",
       "4                  1                       1             Roslindale   \n",
       "\n",
       "    latitude  longitude property_type        room_type  accommodates  \\\n",
       "0  42.282619 -71.133068         House  Entire home/apt             4   \n",
       "1  42.286241 -71.134374     Apartment     Private room             2   \n",
       "2  42.292438 -71.135765     Apartment     Private room             2   \n",
       "3  42.281106 -71.121021         House     Private room             4   \n",
       "4  42.284512 -71.136258         House     Private room             2   \n",
       "\n",
       "   bathrooms  bedrooms  ...  guests_included price_per_extra_person  \\\n",
       "0        1.5       2.0  ...                1                      0   \n",
       "1        1.0       1.0  ...                0                      0   \n",
       "2        1.0       1.0  ...                1                     20   \n",
       "3        1.0       1.0  ...                2                     25   \n",
       "4        1.5       1.0  ...                1                      0   \n",
       "\n",
       "   minimum_nights  number_of_reviews  number_days_btw_first_last_review  \\\n",
       "0               2                  0                                  0   \n",
       "1               2                 36                                804   \n",
       "2               3                 41                               2574   \n",
       "3               1                  1                                  0   \n",
       "4               2                 29                                380   \n",
       "\n",
       "   review_scores_rating  cancellation_policy  price  price_gte_150  \\\n",
       "0                   NaN             moderate    250              1   \n",
       "1                  94.0             moderate     65              0   \n",
       "2                  98.0             moderate     65              0   \n",
       "3                 100.0             moderate     75              0   \n",
       "4                  99.0             flexible     79              0   \n",
       "\n",
       "  price_category  \n",
       "0        gte_226  \n",
       "1        lte_$75  \n",
       "2        lte_$75  \n",
       "3        lte_$75  \n",
       "4   btw_$75-$150  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#We will predict the \"price_gte_150\" value in the data set:\n",
    "\n",
    "airbnb = pd.read_csv(\"airbnb.csv\")\n",
    "airbnb.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_set, test_set = train_test_split(airbnb, test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Be careful: we haven't seperated the target column yet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "host_is_superhost                      0\n",
       "host_identity_verified                 0\n",
       "neighbourhood_cleansed                 0\n",
       "latitude                               0\n",
       "longitude                              0\n",
       "property_type                          2\n",
       "room_type                              0\n",
       "accommodates                           0\n",
       "bathrooms                             10\n",
       "bedrooms                               8\n",
       "beds                                   6\n",
       "bed_type                               0\n",
       "Number of amenities                    0\n",
       "guests_included                        0\n",
       "price_per_extra_person                 0\n",
       "minimum_nights                         0\n",
       "number_of_reviews                      0\n",
       "number_days_btw_first_last_review      0\n",
       "review_scores_rating                 556\n",
       "cancellation_policy                    0\n",
       "price                                  0\n",
       "price_gte_150                          0\n",
       "price_category                         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "host_is_superhost                      0\n",
       "host_identity_verified                 0\n",
       "neighbourhood_cleansed                 0\n",
       "latitude                               0\n",
       "longitude                              0\n",
       "property_type                          1\n",
       "room_type                              0\n",
       "accommodates                           0\n",
       "bathrooms                              4\n",
       "bedrooms                               2\n",
       "beds                                   3\n",
       "bed_type                               0\n",
       "Number of amenities                    0\n",
       "guests_included                        0\n",
       "price_per_extra_person                 0\n",
       "minimum_nights                         0\n",
       "number_of_reviews                      0\n",
       "number_days_btw_first_last_review      0\n",
       "review_scores_rating                 244\n",
       "cancellation_policy                    0\n",
       "price                                  0\n",
       "price_gte_150                          0\n",
       "price_category                         0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "from sklearn.preprocessing import FunctionTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop the variables we can't use in this tutorial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can't use the following columns in this tutorial, because they are not for binary classification tasks\n",
    "\n",
    "train = train_set.drop(['price', 'price_gte_150'], axis=1)\n",
    "test = test_set.drop(['price', 'price_gte_150'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Separate the target variable (we don't want to transform it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_y = train['price_category']\n",
    "test_y = test['price_category']\n",
    "\n",
    "train_inputs = train.drop(['price_category'], axis=1)\n",
    "test_inputs = test.drop(['price_category'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering: Let's derive a new column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    2488.000000\n",
       "mean        2.994775\n",
       "std         7.105866\n",
       "min         1.000000\n",
       "25%         1.000000\n",
       "50%         2.000000\n",
       "75%         3.000000\n",
       "max       273.000000\n",
       "Name: minimum_nights, dtype: float64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's analyze \"minimum_nights\"\n",
    "\n",
    "train_inputs['minimum_nights'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1      1017\n",
       "2       666\n",
       "3       443\n",
       "4        88\n",
       "7        82\n",
       "5        58\n",
       "10       45\n",
       "30       18\n",
       "14       16\n",
       "15       14\n",
       "6        12\n",
       "28        6\n",
       "20        5\n",
       "32        3\n",
       "60        3\n",
       "9         2\n",
       "18        1\n",
       "13        1\n",
       "8         1\n",
       "273       1\n",
       "11        1\n",
       "21        1\n",
       "90        1\n",
       "23        1\n",
       "17        1\n",
       "25        1\n",
       "Name: minimum_nights, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs['minimum_nights'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_inputs['minimum_nights'].hist(bins=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a new categorical column: convert the minimum nights into 4 categories, equally distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1965    (0.999, 2.0]\n",
       "1450    (0.999, 2.0]\n",
       "2503    (3.0, 273.0]\n",
       "944     (0.999, 2.0]\n",
       "199     (0.999, 2.0]\n",
       "            ...     \n",
       "1130    (0.999, 2.0]\n",
       "1294    (3.0, 273.0]\n",
       "860     (0.999, 2.0]\n",
       "3507    (0.999, 2.0]\n",
       "3174    (0.999, 2.0]\n",
       "Name: minimum_nights, Length: 2488, dtype: category\n",
       "Categories (3, interval[float64, right]): [(0.999, 2.0] < (2.0, 3.0] < (3.0, 273.0]]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# qcut creates n quantiles (it is a discretization technique)\n",
    "# Let's create 5 quantiles:\n",
    "pd.qcut(train_inputs['minimum_nights'], 5, duplicates='drop')   \n",
    "\n",
    "# Notice, it can't do 5 quantiles because of the overlapping edges. It can only 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "low       1683\n",
       "medium     443\n",
       "high       362\n",
       "Name: minimum_nights, dtype: int64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.qcut(train_inputs['minimum_nights'],5, duplicates='drop',\n",
    "        labels=['low', 'medium', 'high']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_col(df):\n",
    "    #Create a copy so that we don't overwrite the existing dataframe\n",
    "    df1 = df.copy()\n",
    "    \n",
    "    df1['quantile_min_nights'] = pd.qcut(df1['minimum_nights'],5, duplicates='drop',\n",
    "                        labels=['low', 'medium', 'high'])\n",
    "    \n",
    "\n",
    "    return df1[['quantile_min_nights']]\n",
    "    # You can use this to check whether the calculation is made correctly:\n",
    "    #return df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Identify the numerical and categorical columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "host_is_superhost                      int64\n",
       "host_identity_verified                 int64\n",
       "neighbourhood_cleansed                object\n",
       "latitude                             float64\n",
       "longitude                            float64\n",
       "property_type                         object\n",
       "room_type                             object\n",
       "accommodates                           int64\n",
       "bathrooms                            float64\n",
       "bedrooms                             float64\n",
       "beds                                 float64\n",
       "bed_type                              object\n",
       "Number of amenities                    int64\n",
       "guests_included                        int64\n",
       "price_per_extra_person                 int64\n",
       "minimum_nights                         int64\n",
       "number_of_reviews                      int64\n",
       "number_days_btw_first_last_review      int64\n",
       "review_scores_rating                 float64\n",
       "cancellation_policy                   object\n",
       "dtype: object"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_inputs.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**At this stage, you can manually identify numeric, binary, and categorical columns as follows:**\n",
    "\n",
    "`numeric_columns = ['latitude', 'longitude', 'accommodates', 'bathrooms', 'bedrooms', 'beds', 'Number of amenities', 'guests_included', 'price_per_extra_person', 'minimum_nights', 'number_of_reviews', 'number_days_btw_first_last_review', 'review_scores_rating']`\n",
    " \n",
    " `binary_columns = ['host_is_superhost', 'host_identity_verified']`\n",
    " \n",
    " `categorical_columns = ['neighbourhood_cleansed', 'property_type', 'room_type', 'bed_type', 'cancellation_policy']`\n",
    " \n",
    "<br>\n",
    " \n",
    "**If you do not want to manually type these, you can do the below tricks:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the numerical columns\n",
    "numeric_columns = train_inputs.select_dtypes(include=[np.number]).columns.to_list()\n",
    "\n",
    "# Identify the categorical columns\n",
    "categorical_columns = train_inputs.select_dtypes('object').columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the binary columns so we can pass them through without transforming\n",
    "binary_columns = ['host_is_superhost', 'host_identity_verified']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Be careful: numerical columns already includes the binary columns,\n",
    "# So, we need to remove the binary columns from numerical columns.\n",
    "\n",
    "for col in binary_columns:\n",
    "    numeric_columns.remove(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['host_is_superhost', 'host_identity_verified']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "binary_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['latitude',\n",
       " 'longitude',\n",
       " 'accommodates',\n",
       " 'bathrooms',\n",
       " 'bedrooms',\n",
       " 'beds',\n",
       " 'Number of amenities',\n",
       " 'guests_included',\n",
       " 'price_per_extra_person',\n",
       " 'minimum_nights',\n",
       " 'number_of_reviews',\n",
       " 'number_days_btw_first_last_review',\n",
       " 'review_scores_rating']"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['neighbourhood_cleansed',\n",
       " 'property_type',\n",
       " 'room_type',\n",
       " 'bed_type',\n",
       " 'cancellation_policy']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_eng_columns = ['minimum_nights']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_transformer = Pipeline(steps=[\n",
    "                ('imputer', SimpleImputer(strategy='median')),\n",
    "                ('scaler', StandardScaler())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='unknown')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_new_column = Pipeline(steps=[('my_new_column', FunctionTransformer(new_col)),\n",
    "                               ('imputer', SimpleImputer(strategy='constant', fill_value='unknown')),\n",
    "                               ('onehot', OneHotEncoder(handle_unknown='ignore'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer([\n",
    "        ('num', numeric_transformer, numeric_columns),\n",
    "        ('cat', categorical_transformer, categorical_columns),\n",
    "        ('binary', binary_transformer, binary_columns),\n",
    "        ('trans', my_new_column, feat_eng_columns)],\n",
    "        remainder='passthrough')\n",
    "\n",
    "#passtrough is an optional step. You don't have to use it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform: fit_transform() for TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.82254842,  0.69215829,  0.54753414, ...,  0.        ,\n",
       "         1.        ,  0.        ],\n",
       "       [ 0.55146572,  0.15729058,  0.54753414, ...,  0.        ,\n",
       "         1.        ,  0.        ],\n",
       "       [ 0.07311286, -1.97951247, -0.59100739, ...,  1.        ,\n",
       "         0.        ,  0.        ],\n",
       "       ...,\n",
       "       [-0.61093878, -0.07631528,  3.96315871, ...,  0.        ,\n",
       "         1.        ,  0.        ],\n",
       "       [ 1.17819153, -0.94575177, -1.16027815, ...,  0.        ,\n",
       "         1.        ,  0.        ],\n",
       "       [-0.33618088,  1.03587419, -0.59100739, ...,  0.        ,\n",
       "         1.        ,  0.        ]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Fit and transform the train data\n",
    "train_x = preprocessor.fit_transform(train_inputs)\n",
    "\n",
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2488, 69)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tranform: transform() for TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.21269719, -1.20324989,  0.54753414, ...,  0.        ,\n",
       "         1.        ,  0.        ],\n",
       "       [-2.86419979, -2.67831359, -0.59100739, ...,  0.        ,\n",
       "         1.        ,  0.        ],\n",
       "       [-0.11443035,  1.26295963, -0.59100739, ...,  0.        ,\n",
       "         1.        ,  0.        ],\n",
       "       ...,\n",
       "       [ 0.47803436, -1.63486781, -0.59100739, ...,  0.        ,\n",
       "         1.        ,  0.        ],\n",
       "       [ 0.59928397,  0.34795157,  2.82461719, ...,  0.        ,\n",
       "         1.        ,  0.        ],\n",
       "       [ 0.19953968,  0.22845713, -0.59100739, ...,  1.        ,\n",
       "         0.        ,  0.        ]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform the test data\n",
    "test_x = preprocessor.transform(test_inputs)\n",
    "\n",
    "test_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1067, 69)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "btw_$75-$150     0.331190\n",
       "gte_226          0.235531\n",
       "btw_$151-$225    0.234325\n",
       "lte_$75          0.198955\n",
       "Name: price_category, dtype: float64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find percentage\n",
    "train_y.value_counts()/len(train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline - new approach (not included in the tutorial video)\n",
    "\n",
    "**This section is not included in the tutorial video. Though, it is a better (and a more foolproof) way of calculating the baseline. I recommend using this approach rather than the above approach.**\n",
    "\n",
    "**Note that it generates the same results as the above approach.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DummyClassifier(strategy=&#x27;most_frequent&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DummyClassifier</label><div class=\"sk-toggleable__content\"><pre>DummyClassifier(strategy=&#x27;most_frequent&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "DummyClassifier(strategy='most_frequent')"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sci-kit Learn's DummyClassifier automates the entire process performed manually above.\n",
    "# First, you instantiate it with the \"most_frequent\" strategy. \n",
    "# Then, you fit the model. This finds the most frequently (i.e., majority) class (in the training set).\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "dummy_clf = DummyClassifier(strategy=\"most_frequent\")\n",
    "\n",
    "dummy_clf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Train Accuracy: 0.3311897106109325\n"
     ]
    }
   ],
   "source": [
    "# Then, you call the predict function of the classifier.\n",
    "# This predicts all values as the majority class.\n",
    "# In the next step, you compare the predicted values with the actual values to calculate accuracy\n",
    "# This is the baseline Train Accuracy\n",
    "\n",
    "dummy_train_pred = dummy_clf.predict(train_x)\n",
    "\n",
    "baseline_train_acc = accuracy_score(train_y, dummy_train_pred)\n",
    "\n",
    "print('Baseline Train Accuracy: {}' .format(baseline_train_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Test Accuracy: 0.3402061855670103\n"
     ]
    }
   ],
   "source": [
    "# We repeat the same steps for the test set\n",
    "# This is the baseline Test Accuracy\n",
    "\n",
    "dummy_test_pred = dummy_clf.predict(test_x)\n",
    "\n",
    "baseline_test_acc = accuracy_score(test_y, dummy_test_pred)\n",
    "\n",
    "print('Baseline Test Accuracy: {}' .format(baseline_test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass classification\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mural\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (200) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-4 {color: black;}#sk-container-id-4 pre{padding: 0;}#sk-container-id-4 div.sk-toggleable {background-color: white;}#sk-container-id-4 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-4 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-4 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-4 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-4 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-4 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-4 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-4 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-4 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-4 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-4 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-4 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-4 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-4 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-4 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-4 div.sk-item {position: relative;z-index: 1;}#sk-container-id-4 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-4 div.sk-item::before, #sk-container-id-4 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-4 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-4 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-4 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-4 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-4 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-4 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-4 div.sk-label-container {text-align: center;}#sk-container-id-4 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-4 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-4\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" checked><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier()"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "#Default settings create 1 hidden layer with 100 neurons\n",
    "mlp_clf = MLPClassifier(hidden_layer_sizes=(100,))\n",
    "\n",
    "mlp_clf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.889871382636656"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict the train values\n",
    "train_y_pred = mlp_clf.predict(train_x)\n",
    "\n",
    "#Train accuracy\n",
    "accuracy_score(train_y, train_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6232427366447985"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict the test values\n",
    "test_y_pred = mlp_clf.predict(test_x)\n",
    "\n",
    "#Test accuracy\n",
    "accuracy_score(test_y, test_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[107,  52,  85,   1],\n",
       "       [ 43, 226,  24,  70],\n",
       "       [ 49,  23, 149,   0],\n",
       "       [ 10,  44,   1, 183]], dtype=int64)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "#We usually create the confusion matrix on test set\n",
    "confusion_matrix(test_y, test_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Increase maximum iterations for convergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.41182652\n",
      "Iteration 2, loss = 1.19273074\n",
      "Iteration 3, loss = 1.09279648\n",
      "Iteration 4, loss = 1.02829395\n",
      "Iteration 5, loss = 0.98236597\n",
      "Iteration 6, loss = 0.94773749\n",
      "Iteration 7, loss = 0.92050194\n",
      "Iteration 8, loss = 0.89842055\n",
      "Iteration 9, loss = 0.87920322\n",
      "Iteration 10, loss = 0.86334943\n",
      "Iteration 11, loss = 0.84966031\n",
      "Iteration 12, loss = 0.83716728\n",
      "Iteration 13, loss = 0.82640662\n",
      "Iteration 14, loss = 0.81719555\n",
      "Iteration 15, loss = 0.80887793\n",
      "Iteration 16, loss = 0.80008048\n",
      "Iteration 17, loss = 0.79306607\n",
      "Iteration 18, loss = 0.78569594\n",
      "Iteration 19, loss = 0.77949117\n",
      "Iteration 20, loss = 0.77211929\n",
      "Iteration 21, loss = 0.76703975\n",
      "Iteration 22, loss = 0.76137903\n",
      "Iteration 23, loss = 0.75565969\n",
      "Iteration 24, loss = 0.75086442\n",
      "Iteration 25, loss = 0.74575198\n",
      "Iteration 26, loss = 0.73963924\n",
      "Iteration 27, loss = 0.73658884\n",
      "Iteration 28, loss = 0.73152358\n",
      "Iteration 29, loss = 0.72652639\n",
      "Iteration 30, loss = 0.72155757\n",
      "Iteration 31, loss = 0.71806350\n",
      "Iteration 32, loss = 0.71377901\n",
      "Iteration 33, loss = 0.70881262\n",
      "Iteration 34, loss = 0.70521936\n",
      "Iteration 35, loss = 0.70035595\n",
      "Iteration 36, loss = 0.69639397\n",
      "Iteration 37, loss = 0.69347432\n",
      "Iteration 38, loss = 0.68928673\n",
      "Iteration 39, loss = 0.68553276\n",
      "Iteration 40, loss = 0.68148024\n",
      "Iteration 41, loss = 0.67822109\n",
      "Iteration 42, loss = 0.67444215\n",
      "Iteration 43, loss = 0.67045282\n",
      "Iteration 44, loss = 0.66740992\n",
      "Iteration 45, loss = 0.66214226\n",
      "Iteration 46, loss = 0.65850910\n",
      "Iteration 47, loss = 0.65535344\n",
      "Iteration 48, loss = 0.65197774\n",
      "Iteration 49, loss = 0.64823538\n",
      "Iteration 50, loss = 0.64524937\n",
      "Iteration 51, loss = 0.64088411\n",
      "Iteration 52, loss = 0.63818563\n",
      "Iteration 53, loss = 0.63576210\n",
      "Iteration 54, loss = 0.63215151\n",
      "Iteration 55, loss = 0.62803563\n",
      "Iteration 56, loss = 0.62537186\n",
      "Iteration 57, loss = 0.62181040\n",
      "Iteration 58, loss = 0.61746104\n",
      "Iteration 59, loss = 0.61473287\n",
      "Iteration 60, loss = 0.61205652\n",
      "Iteration 61, loss = 0.60816610\n",
      "Iteration 62, loss = 0.60504142\n",
      "Iteration 63, loss = 0.60333103\n",
      "Iteration 64, loss = 0.60031974\n",
      "Iteration 65, loss = 0.59629451\n",
      "Iteration 66, loss = 0.59224240\n",
      "Iteration 67, loss = 0.59047705\n",
      "Iteration 68, loss = 0.58807833\n",
      "Iteration 69, loss = 0.58313491\n",
      "Iteration 70, loss = 0.58012366\n",
      "Iteration 71, loss = 0.57816678\n",
      "Iteration 72, loss = 0.57560177\n",
      "Iteration 73, loss = 0.57298100\n",
      "Iteration 74, loss = 0.56894906\n",
      "Iteration 75, loss = 0.56708590\n",
      "Iteration 76, loss = 0.56251226\n",
      "Iteration 77, loss = 0.56022124\n",
      "Iteration 78, loss = 0.55743741\n",
      "Iteration 79, loss = 0.55505504\n",
      "Iteration 80, loss = 0.55197081\n",
      "Iteration 81, loss = 0.54977132\n",
      "Iteration 82, loss = 0.54731812\n",
      "Iteration 83, loss = 0.54544020\n",
      "Iteration 84, loss = 0.54355190\n",
      "Iteration 85, loss = 0.54133841\n",
      "Iteration 86, loss = 0.53686525\n",
      "Iteration 87, loss = 0.53564323\n",
      "Iteration 88, loss = 0.53206730\n",
      "Iteration 89, loss = 0.52954364\n",
      "Iteration 90, loss = 0.52656553\n",
      "Iteration 91, loss = 0.52417773\n",
      "Iteration 92, loss = 0.52145759\n",
      "Iteration 93, loss = 0.51875861\n",
      "Iteration 94, loss = 0.51672271\n",
      "Iteration 95, loss = 0.51344225\n",
      "Iteration 96, loss = 0.51194809\n",
      "Iteration 97, loss = 0.51068454\n",
      "Iteration 98, loss = 0.50997777\n",
      "Iteration 99, loss = 0.50467129\n",
      "Iteration 100, loss = 0.50156618\n",
      "Iteration 101, loss = 0.50062448\n",
      "Iteration 102, loss = 0.49759173\n",
      "Iteration 103, loss = 0.49547128\n",
      "Iteration 104, loss = 0.49286618\n",
      "Iteration 105, loss = 0.49143496\n",
      "Iteration 106, loss = 0.48881808\n",
      "Iteration 107, loss = 0.48692282\n",
      "Iteration 108, loss = 0.48360321\n",
      "Iteration 109, loss = 0.48135627\n",
      "Iteration 110, loss = 0.47996510\n",
      "Iteration 111, loss = 0.47690349\n",
      "Iteration 112, loss = 0.47464911\n",
      "Iteration 113, loss = 0.47317716\n",
      "Iteration 114, loss = 0.47026336\n",
      "Iteration 115, loss = 0.46893560\n",
      "Iteration 116, loss = 0.46808822\n",
      "Iteration 117, loss = 0.46526733\n",
      "Iteration 118, loss = 0.46470691\n",
      "Iteration 119, loss = 0.46164434\n",
      "Iteration 120, loss = 0.45863041\n",
      "Iteration 121, loss = 0.45725931\n",
      "Iteration 122, loss = 0.45425492\n",
      "Iteration 123, loss = 0.45293189\n",
      "Iteration 124, loss = 0.45179167\n",
      "Iteration 125, loss = 0.44957315\n",
      "Iteration 126, loss = 0.44657740\n",
      "Iteration 127, loss = 0.44494677\n",
      "Iteration 128, loss = 0.44357896\n",
      "Iteration 129, loss = 0.44191545\n",
      "Iteration 130, loss = 0.43877067\n",
      "Iteration 131, loss = 0.43764112\n",
      "Iteration 132, loss = 0.43677004\n",
      "Iteration 133, loss = 0.43439512\n",
      "Iteration 134, loss = 0.43349097\n",
      "Iteration 135, loss = 0.43209080\n",
      "Iteration 136, loss = 0.43008732\n",
      "Iteration 137, loss = 0.42801229\n",
      "Iteration 138, loss = 0.42680284\n",
      "Iteration 139, loss = 0.42301065\n",
      "Iteration 140, loss = 0.42142950\n",
      "Iteration 141, loss = 0.41936909\n",
      "Iteration 142, loss = 0.41895262\n",
      "Iteration 143, loss = 0.41539923\n",
      "Iteration 144, loss = 0.41362829\n",
      "Iteration 145, loss = 0.41434076\n",
      "Iteration 146, loss = 0.41147691\n",
      "Iteration 147, loss = 0.40853804\n",
      "Iteration 148, loss = 0.40716412\n",
      "Iteration 149, loss = 0.40603664\n",
      "Iteration 150, loss = 0.40356507\n",
      "Iteration 151, loss = 0.40113082\n",
      "Iteration 152, loss = 0.40064372\n",
      "Iteration 153, loss = 0.39834314\n",
      "Iteration 154, loss = 0.39782272\n",
      "Iteration 155, loss = 0.39557485\n",
      "Iteration 156, loss = 0.39391349\n",
      "Iteration 157, loss = 0.39160292\n",
      "Iteration 158, loss = 0.39096275\n",
      "Iteration 159, loss = 0.38959175\n",
      "Iteration 160, loss = 0.38762799\n",
      "Iteration 161, loss = 0.38605778\n",
      "Iteration 162, loss = 0.38478504\n",
      "Iteration 163, loss = 0.38325171\n",
      "Iteration 164, loss = 0.38128312\n",
      "Iteration 165, loss = 0.37996436\n",
      "Iteration 166, loss = 0.37957222\n",
      "Iteration 167, loss = 0.37702024\n",
      "Iteration 168, loss = 0.37547568\n",
      "Iteration 169, loss = 0.37535193\n",
      "Iteration 170, loss = 0.37426082\n",
      "Iteration 171, loss = 0.37214593\n",
      "Iteration 172, loss = 0.37041226\n",
      "Iteration 173, loss = 0.36909804\n",
      "Iteration 174, loss = 0.36569720\n",
      "Iteration 175, loss = 0.36567749\n",
      "Iteration 176, loss = 0.36387627\n",
      "Iteration 177, loss = 0.36467160\n",
      "Iteration 178, loss = 0.36100610\n",
      "Iteration 179, loss = 0.35933713\n",
      "Iteration 180, loss = 0.36002945\n",
      "Iteration 181, loss = 0.35729642\n",
      "Iteration 182, loss = 0.35371528\n",
      "Iteration 183, loss = 0.35410574\n",
      "Iteration 184, loss = 0.35217047\n",
      "Iteration 185, loss = 0.35085252\n",
      "Iteration 186, loss = 0.34866910\n",
      "Iteration 187, loss = 0.34805564\n",
      "Iteration 188, loss = 0.34604051\n",
      "Iteration 189, loss = 0.34431970\n",
      "Iteration 190, loss = 0.34411266\n",
      "Iteration 191, loss = 0.34145771\n",
      "Iteration 192, loss = 0.33993564\n",
      "Iteration 193, loss = 0.33992810\n",
      "Iteration 194, loss = 0.33830161\n",
      "Iteration 195, loss = 0.33702951\n",
      "Iteration 196, loss = 0.33512385\n",
      "Iteration 197, loss = 0.33447254\n",
      "Iteration 198, loss = 0.33275370\n",
      "Iteration 199, loss = 0.33163463\n",
      "Iteration 200, loss = 0.32983624\n",
      "Iteration 201, loss = 0.32888168\n",
      "Iteration 202, loss = 0.32772385\n",
      "Iteration 203, loss = 0.32688116\n",
      "Iteration 204, loss = 0.32506824\n",
      "Iteration 205, loss = 0.32621975\n",
      "Iteration 206, loss = 0.32297399\n",
      "Iteration 207, loss = 0.32226588\n",
      "Iteration 208, loss = 0.32066950\n",
      "Iteration 209, loss = 0.31853678\n",
      "Iteration 210, loss = 0.31832171\n",
      "Iteration 211, loss = 0.31644259\n",
      "Iteration 212, loss = 0.31550279\n",
      "Iteration 213, loss = 0.31390044\n",
      "Iteration 214, loss = 0.31437020\n",
      "Iteration 215, loss = 0.31476898\n",
      "Iteration 216, loss = 0.31201826\n",
      "Iteration 217, loss = 0.31177066\n",
      "Iteration 218, loss = 0.31034807\n",
      "Iteration 219, loss = 0.30793453\n",
      "Iteration 220, loss = 0.30683896\n",
      "Iteration 221, loss = 0.30500713\n",
      "Iteration 222, loss = 0.30349249\n",
      "Iteration 223, loss = 0.30116499\n",
      "Iteration 224, loss = 0.30066388\n",
      "Iteration 225, loss = 0.30015691\n",
      "Iteration 226, loss = 0.29934515\n",
      "Iteration 227, loss = 0.29856431\n",
      "Iteration 228, loss = 0.29585902\n",
      "Iteration 229, loss = 0.29621301\n",
      "Iteration 230, loss = 0.29592993\n",
      "Iteration 231, loss = 0.29548543\n",
      "Iteration 232, loss = 0.29528357\n",
      "Iteration 233, loss = 0.29409921\n",
      "Iteration 234, loss = 0.29192671\n",
      "Iteration 235, loss = 0.28894730\n",
      "Iteration 236, loss = 0.28718967\n",
      "Iteration 237, loss = 0.28637479\n",
      "Iteration 238, loss = 0.28525713\n",
      "Iteration 239, loss = 0.28428899\n",
      "Iteration 240, loss = 0.28459034\n",
      "Iteration 241, loss = 0.28237082\n",
      "Iteration 242, loss = 0.28115646\n",
      "Iteration 243, loss = 0.28171650\n",
      "Iteration 244, loss = 0.27850305\n",
      "Iteration 245, loss = 0.27932666\n",
      "Iteration 246, loss = 0.27784891\n",
      "Iteration 247, loss = 0.27652100\n",
      "Iteration 248, loss = 0.27712142\n",
      "Iteration 249, loss = 0.27610821\n",
      "Iteration 250, loss = 0.27469935\n",
      "Iteration 251, loss = 0.27418152\n",
      "Iteration 252, loss = 0.27152370\n",
      "Iteration 253, loss = 0.26980492\n",
      "Iteration 254, loss = 0.26984932\n",
      "Iteration 255, loss = 0.26871677\n",
      "Iteration 256, loss = 0.26744347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 257, loss = 0.26660347\n",
      "Iteration 258, loss = 0.26627758\n",
      "Iteration 259, loss = 0.26463406\n",
      "Iteration 260, loss = 0.26314481\n",
      "Iteration 261, loss = 0.26255726\n",
      "Iteration 262, loss = 0.26210489\n",
      "Iteration 263, loss = 0.26113856\n",
      "Iteration 264, loss = 0.26057159\n",
      "Iteration 265, loss = 0.25936012\n",
      "Iteration 266, loss = 0.25875773\n",
      "Iteration 267, loss = 0.25833179\n",
      "Iteration 268, loss = 0.25720297\n",
      "Iteration 269, loss = 0.25502319\n",
      "Iteration 270, loss = 0.25321772\n",
      "Iteration 271, loss = 0.25246230\n",
      "Iteration 272, loss = 0.25331932\n",
      "Iteration 273, loss = 0.25214793\n",
      "Iteration 274, loss = 0.25270007\n",
      "Iteration 275, loss = 0.25137415\n",
      "Iteration 276, loss = 0.24853276\n",
      "Iteration 277, loss = 0.24801670\n",
      "Iteration 278, loss = 0.24817618\n",
      "Iteration 279, loss = 0.24535536\n",
      "Iteration 280, loss = 0.24640540\n",
      "Iteration 281, loss = 0.24490911\n",
      "Iteration 282, loss = 0.24408382\n",
      "Iteration 283, loss = 0.24322926\n",
      "Iteration 284, loss = 0.24248740\n",
      "Iteration 285, loss = 0.24134384\n",
      "Iteration 286, loss = 0.24048464\n",
      "Iteration 287, loss = 0.24018262\n",
      "Iteration 288, loss = 0.24012803\n",
      "Iteration 289, loss = 0.24014816\n",
      "Iteration 290, loss = 0.23923788\n",
      "Iteration 291, loss = 0.23783820\n",
      "Iteration 292, loss = 0.23500805\n",
      "Iteration 293, loss = 0.23530972\n",
      "Iteration 294, loss = 0.23350587\n",
      "Iteration 295, loss = 0.23233649\n",
      "Iteration 296, loss = 0.23222622\n",
      "Iteration 297, loss = 0.23157842\n",
      "Iteration 298, loss = 0.23046128\n",
      "Iteration 299, loss = 0.22952625\n",
      "Iteration 300, loss = 0.22931830\n",
      "Iteration 301, loss = 0.22900523\n",
      "Iteration 302, loss = 0.22891253\n",
      "Iteration 303, loss = 0.22700240\n",
      "Iteration 304, loss = 0.22457951\n",
      "Iteration 305, loss = 0.22505671\n",
      "Iteration 306, loss = 0.22420173\n",
      "Iteration 307, loss = 0.22453452\n",
      "Iteration 308, loss = 0.22533266\n",
      "Iteration 309, loss = 0.22205942\n",
      "Iteration 310, loss = 0.22134574\n",
      "Iteration 311, loss = 0.21944402\n",
      "Iteration 312, loss = 0.21991466\n",
      "Iteration 313, loss = 0.21788853\n",
      "Iteration 314, loss = 0.21769380\n",
      "Iteration 315, loss = 0.21672452\n",
      "Iteration 316, loss = 0.21556113\n",
      "Iteration 317, loss = 0.21584197\n",
      "Iteration 318, loss = 0.21590070\n",
      "Iteration 319, loss = 0.21315435\n",
      "Iteration 320, loss = 0.21235765\n",
      "Iteration 321, loss = 0.21323904\n",
      "Iteration 322, loss = 0.21130977\n",
      "Iteration 323, loss = 0.21048224\n",
      "Iteration 324, loss = 0.20966873\n",
      "Iteration 325, loss = 0.20913540\n",
      "Iteration 326, loss = 0.21002273\n",
      "Iteration 327, loss = 0.20974906\n",
      "Iteration 328, loss = 0.20936403\n",
      "Iteration 329, loss = 0.20708551\n",
      "Iteration 330, loss = 0.20677305\n",
      "Iteration 331, loss = 0.20537457\n",
      "Iteration 332, loss = 0.20343755\n",
      "Iteration 333, loss = 0.20333293\n",
      "Iteration 334, loss = 0.20466683\n",
      "Iteration 335, loss = 0.20401869\n",
      "Iteration 336, loss = 0.20259451\n",
      "Iteration 337, loss = 0.20178812\n",
      "Iteration 338, loss = 0.20108026\n",
      "Iteration 339, loss = 0.20024458\n",
      "Iteration 340, loss = 0.19869069\n",
      "Iteration 341, loss = 0.20010197\n",
      "Iteration 342, loss = 0.20023226\n",
      "Iteration 343, loss = 0.19794813\n",
      "Iteration 344, loss = 0.19656333\n",
      "Iteration 345, loss = 0.19529807\n",
      "Iteration 346, loss = 0.19652164\n",
      "Iteration 347, loss = 0.19684089\n",
      "Iteration 348, loss = 0.19617773\n",
      "Iteration 349, loss = 0.19550617\n",
      "Iteration 350, loss = 0.19246582\n",
      "Iteration 351, loss = 0.19272875\n",
      "Iteration 352, loss = 0.19046382\n",
      "Iteration 353, loss = 0.18956713\n",
      "Iteration 354, loss = 0.18953387\n",
      "Iteration 355, loss = 0.18988304\n",
      "Iteration 356, loss = 0.18828618\n",
      "Iteration 357, loss = 0.18795438\n",
      "Iteration 358, loss = 0.18781163\n",
      "Iteration 359, loss = 0.18805209\n",
      "Iteration 360, loss = 0.18605394\n",
      "Iteration 361, loss = 0.18549992\n",
      "Iteration 362, loss = 0.18953445\n",
      "Iteration 363, loss = 0.18622128\n",
      "Iteration 364, loss = 0.18426394\n",
      "Iteration 365, loss = 0.18381355\n",
      "Iteration 366, loss = 0.18132376\n",
      "Iteration 367, loss = 0.18098329\n",
      "Iteration 368, loss = 0.18106095\n",
      "Iteration 369, loss = 0.17957166\n",
      "Iteration 370, loss = 0.18082115\n",
      "Iteration 371, loss = 0.17898868\n",
      "Iteration 372, loss = 0.17989885\n",
      "Iteration 373, loss = 0.17722896\n",
      "Iteration 374, loss = 0.17800383\n",
      "Iteration 375, loss = 0.17706610\n",
      "Iteration 376, loss = 0.17678909\n",
      "Iteration 377, loss = 0.17536951\n",
      "Iteration 378, loss = 0.17550548\n",
      "Iteration 379, loss = 0.17608387\n",
      "Iteration 380, loss = 0.17257062\n",
      "Iteration 381, loss = 0.17443903\n",
      "Iteration 382, loss = 0.17216002\n",
      "Iteration 383, loss = 0.17231014\n",
      "Iteration 384, loss = 0.17167926\n",
      "Iteration 385, loss = 0.17065103\n",
      "Iteration 386, loss = 0.17039023\n",
      "Iteration 387, loss = 0.16902391\n",
      "Iteration 388, loss = 0.16961899\n",
      "Iteration 389, loss = 0.16854570\n",
      "Iteration 390, loss = 0.16802010\n",
      "Iteration 391, loss = 0.16786717\n",
      "Iteration 392, loss = 0.16642945\n",
      "Iteration 393, loss = 0.16783639\n",
      "Iteration 394, loss = 0.16622253\n",
      "Iteration 395, loss = 0.16541339\n",
      "Iteration 396, loss = 0.16598693\n",
      "Iteration 397, loss = 0.16439190\n",
      "Iteration 398, loss = 0.16452176\n",
      "Iteration 399, loss = 0.16362726\n",
      "Iteration 400, loss = 0.16198184\n",
      "Iteration 401, loss = 0.16307048\n",
      "Iteration 402, loss = 0.16138973\n",
      "Iteration 403, loss = 0.16162441\n",
      "Iteration 404, loss = 0.16082519\n",
      "Iteration 405, loss = 0.15981841\n",
      "Iteration 406, loss = 0.16013660\n",
      "Iteration 407, loss = 0.15828998\n",
      "Iteration 408, loss = 0.15787997\n",
      "Iteration 409, loss = 0.15772856\n",
      "Iteration 410, loss = 0.15686925\n",
      "Iteration 411, loss = 0.15642421\n",
      "Iteration 412, loss = 0.15644907\n",
      "Iteration 413, loss = 0.15653189\n",
      "Iteration 414, loss = 0.15582812\n",
      "Iteration 415, loss = 0.15529776\n",
      "Iteration 416, loss = 0.15431183\n",
      "Iteration 417, loss = 0.15299252\n",
      "Iteration 418, loss = 0.15329759\n",
      "Iteration 419, loss = 0.15317676\n",
      "Iteration 420, loss = 0.15198096\n",
      "Iteration 421, loss = 0.15153655\n",
      "Iteration 422, loss = 0.15083001\n",
      "Iteration 423, loss = 0.15091849\n",
      "Iteration 424, loss = 0.15039994\n",
      "Iteration 425, loss = 0.14862854\n",
      "Iteration 426, loss = 0.14892592\n",
      "Iteration 427, loss = 0.14817213\n",
      "Iteration 428, loss = 0.14968614\n",
      "Iteration 429, loss = 0.14788027\n",
      "Iteration 430, loss = 0.14866449\n",
      "Iteration 431, loss = 0.14641037\n",
      "Iteration 432, loss = 0.14684818\n",
      "Iteration 433, loss = 0.14584072\n",
      "Iteration 434, loss = 0.14493799\n",
      "Iteration 435, loss = 0.14446228\n",
      "Iteration 436, loss = 0.14618333\n",
      "Iteration 437, loss = 0.14547561\n",
      "Iteration 438, loss = 0.14397228\n",
      "Iteration 439, loss = 0.14381222\n",
      "Iteration 440, loss = 0.14305326\n",
      "Iteration 441, loss = 0.14219105\n",
      "Iteration 442, loss = 0.14074522\n",
      "Iteration 443, loss = 0.14078711\n",
      "Iteration 444, loss = 0.14017856\n",
      "Iteration 445, loss = 0.14018740\n",
      "Iteration 446, loss = 0.13894690\n",
      "Iteration 447, loss = 0.13892331\n",
      "Iteration 448, loss = 0.13790108\n",
      "Iteration 449, loss = 0.13776274\n",
      "Iteration 450, loss = 0.13803401\n",
      "Iteration 451, loss = 0.13801505\n",
      "Iteration 452, loss = 0.13645585\n",
      "Iteration 453, loss = 0.13611587\n",
      "Iteration 454, loss = 0.13549546\n",
      "Iteration 455, loss = 0.13628742\n",
      "Iteration 456, loss = 0.13496245\n",
      "Iteration 457, loss = 0.13623870\n",
      "Iteration 458, loss = 0.13468145\n",
      "Iteration 459, loss = 0.13604758\n",
      "Iteration 460, loss = 0.13406149\n",
      "Iteration 461, loss = 0.13361276\n",
      "Iteration 462, loss = 0.13175216\n",
      "Iteration 463, loss = 0.13160693\n",
      "Iteration 464, loss = 0.13091054\n",
      "Iteration 465, loss = 0.13221411\n",
      "Iteration 466, loss = 0.13039712\n",
      "Iteration 467, loss = 0.13011913\n",
      "Iteration 468, loss = 0.13004047\n",
      "Iteration 469, loss = 0.13159546\n",
      "Iteration 470, loss = 0.12952867\n",
      "Iteration 471, loss = 0.12838182\n",
      "Iteration 472, loss = 0.12729621\n",
      "Iteration 473, loss = 0.12844225\n",
      "Iteration 474, loss = 0.12802527\n",
      "Iteration 475, loss = 0.12795056\n",
      "Iteration 476, loss = 0.12724142\n",
      "Iteration 477, loss = 0.12632419\n",
      "Iteration 478, loss = 0.12613995\n",
      "Iteration 479, loss = 0.12548671\n",
      "Iteration 480, loss = 0.12510089\n",
      "Iteration 481, loss = 0.12603327\n",
      "Iteration 482, loss = 0.12551460\n",
      "Iteration 483, loss = 0.12445398\n",
      "Iteration 484, loss = 0.12427336\n",
      "Iteration 485, loss = 0.12286192\n",
      "Iteration 486, loss = 0.12329707\n",
      "Iteration 487, loss = 0.12307827\n",
      "Iteration 488, loss = 0.12260789\n",
      "Iteration 489, loss = 0.12291832\n",
      "Iteration 490, loss = 0.12087700\n",
      "Iteration 491, loss = 0.12170788\n",
      "Iteration 492, loss = 0.11990654\n",
      "Iteration 493, loss = 0.12168260\n",
      "Iteration 494, loss = 0.12114741\n",
      "Iteration 495, loss = 0.11911024\n",
      "Iteration 496, loss = 0.11808427\n",
      "Iteration 497, loss = 0.11840153\n",
      "Iteration 498, loss = 0.11762713\n",
      "Iteration 499, loss = 0.11869334\n",
      "Iteration 500, loss = 0.11692291\n",
      "Iteration 501, loss = 0.11758440\n",
      "Iteration 502, loss = 0.11648833\n",
      "Iteration 503, loss = 0.11552232\n",
      "Iteration 504, loss = 0.11650179\n",
      "Iteration 505, loss = 0.11584034\n",
      "Iteration 506, loss = 0.11696733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 507, loss = 0.11593806\n",
      "Iteration 508, loss = 0.11671484\n",
      "Iteration 509, loss = 0.11466431\n",
      "Iteration 510, loss = 0.11286334\n",
      "Iteration 511, loss = 0.11365017\n",
      "Iteration 512, loss = 0.11300268\n",
      "Iteration 513, loss = 0.11335633\n",
      "Iteration 514, loss = 0.11213109\n",
      "Iteration 515, loss = 0.11091638\n",
      "Iteration 516, loss = 0.11245761\n",
      "Iteration 517, loss = 0.11108924\n",
      "Iteration 518, loss = 0.11078783\n",
      "Iteration 519, loss = 0.11089291\n",
      "Iteration 520, loss = 0.10946698\n",
      "Iteration 521, loss = 0.11028311\n",
      "Iteration 522, loss = 0.10993287\n",
      "Iteration 523, loss = 0.10878553\n",
      "Iteration 524, loss = 0.10936977\n",
      "Iteration 525, loss = 0.11009269\n",
      "Iteration 526, loss = 0.10935493\n",
      "Iteration 527, loss = 0.10833026\n",
      "Iteration 528, loss = 0.10827513\n",
      "Iteration 529, loss = 0.10710847\n",
      "Iteration 530, loss = 0.10757313\n",
      "Iteration 531, loss = 0.10674365\n",
      "Iteration 532, loss = 0.10576794\n",
      "Iteration 533, loss = 0.10546778\n",
      "Iteration 534, loss = 0.10550096\n",
      "Iteration 535, loss = 0.10637393\n",
      "Iteration 536, loss = 0.10475258\n",
      "Iteration 537, loss = 0.10449504\n",
      "Iteration 538, loss = 0.10440091\n",
      "Iteration 539, loss = 0.10473015\n",
      "Iteration 540, loss = 0.10441111\n",
      "Iteration 541, loss = 0.10481476\n",
      "Iteration 542, loss = 0.10264273\n",
      "Iteration 543, loss = 0.10216574\n",
      "Iteration 544, loss = 0.10235554\n",
      "Iteration 545, loss = 0.10292273\n",
      "Iteration 546, loss = 0.10246050\n",
      "Iteration 547, loss = 0.10206871\n",
      "Iteration 548, loss = 0.10098816\n",
      "Iteration 549, loss = 0.10124875\n",
      "Iteration 550, loss = 0.10354284\n",
      "Iteration 551, loss = 0.10084016\n",
      "Iteration 552, loss = 0.10110476\n",
      "Iteration 553, loss = 0.10080871\n",
      "Iteration 554, loss = 0.09892809\n",
      "Iteration 555, loss = 0.09881762\n",
      "Iteration 556, loss = 0.09973305\n",
      "Iteration 557, loss = 0.09820510\n",
      "Iteration 558, loss = 0.09949646\n",
      "Iteration 559, loss = 0.09961333\n",
      "Iteration 560, loss = 0.09834902\n",
      "Iteration 561, loss = 0.09679269\n",
      "Iteration 562, loss = 0.09676385\n",
      "Iteration 563, loss = 0.09590783\n",
      "Iteration 564, loss = 0.09671248\n",
      "Iteration 565, loss = 0.09593779\n",
      "Iteration 566, loss = 0.09572706\n",
      "Iteration 567, loss = 0.09461952\n",
      "Iteration 568, loss = 0.09438513\n",
      "Iteration 569, loss = 0.09519369\n",
      "Iteration 570, loss = 0.09552063\n",
      "Iteration 571, loss = 0.09483157\n",
      "Iteration 572, loss = 0.09583305\n",
      "Iteration 573, loss = 0.09515866\n",
      "Iteration 574, loss = 0.09264317\n",
      "Iteration 575, loss = 0.09290423\n",
      "Iteration 576, loss = 0.09309633\n",
      "Iteration 577, loss = 0.09221977\n",
      "Iteration 578, loss = 0.09388581\n",
      "Iteration 579, loss = 0.09235274\n",
      "Iteration 580, loss = 0.09363754\n",
      "Iteration 581, loss = 0.09180435\n",
      "Iteration 582, loss = 0.09424787\n",
      "Iteration 583, loss = 0.08975293\n",
      "Iteration 584, loss = 0.08990659\n",
      "Iteration 585, loss = 0.09006412\n",
      "Iteration 586, loss = 0.08989792\n",
      "Iteration 587, loss = 0.08912404\n",
      "Iteration 588, loss = 0.08814209\n",
      "Iteration 589, loss = 0.08953643\n",
      "Iteration 590, loss = 0.08921672\n",
      "Iteration 591, loss = 0.09018102\n",
      "Iteration 592, loss = 0.08734642\n",
      "Iteration 593, loss = 0.08876603\n",
      "Iteration 594, loss = 0.08889438\n",
      "Iteration 595, loss = 0.08776947\n",
      "Iteration 596, loss = 0.08676789\n",
      "Iteration 597, loss = 0.08698871\n",
      "Iteration 598, loss = 0.08745568\n",
      "Iteration 599, loss = 0.08655826\n",
      "Iteration 600, loss = 0.08612730\n",
      "Iteration 601, loss = 0.08518971\n",
      "Iteration 602, loss = 0.08520969\n",
      "Iteration 603, loss = 0.08520025\n",
      "Iteration 604, loss = 0.08464347\n",
      "Iteration 605, loss = 0.08466295\n",
      "Iteration 606, loss = 0.08340318\n",
      "Iteration 607, loss = 0.08432947\n",
      "Iteration 608, loss = 0.08576333\n",
      "Iteration 609, loss = 0.08389156\n",
      "Iteration 610, loss = 0.08270355\n",
      "Iteration 611, loss = 0.08269054\n",
      "Iteration 612, loss = 0.08166879\n",
      "Iteration 613, loss = 0.08212902\n",
      "Iteration 614, loss = 0.08307490\n",
      "Iteration 615, loss = 0.08240189\n",
      "Iteration 616, loss = 0.08162812\n",
      "Iteration 617, loss = 0.08087545\n",
      "Iteration 618, loss = 0.08172527\n",
      "Iteration 619, loss = 0.08202533\n",
      "Iteration 620, loss = 0.08266997\n",
      "Iteration 621, loss = 0.08175303\n",
      "Iteration 622, loss = 0.08213751\n",
      "Iteration 623, loss = 0.08124969\n",
      "Iteration 624, loss = 0.08174708\n",
      "Iteration 625, loss = 0.07960098\n",
      "Iteration 626, loss = 0.08060027\n",
      "Iteration 627, loss = 0.07982739\n",
      "Iteration 628, loss = 0.07876119\n",
      "Iteration 629, loss = 0.07965959\n",
      "Iteration 630, loss = 0.07961457\n",
      "Iteration 631, loss = 0.07904806\n",
      "Iteration 632, loss = 0.07756134\n",
      "Iteration 633, loss = 0.08096178\n",
      "Iteration 634, loss = 0.07884789\n",
      "Iteration 635, loss = 0.07733049\n",
      "Iteration 636, loss = 0.07687389\n",
      "Iteration 637, loss = 0.07679326\n",
      "Iteration 638, loss = 0.07651551\n",
      "Iteration 639, loss = 0.07671949\n",
      "Iteration 640, loss = 0.07662574\n",
      "Iteration 641, loss = 0.07575686\n",
      "Iteration 642, loss = 0.07632260\n",
      "Iteration 643, loss = 0.07609441\n",
      "Iteration 644, loss = 0.07536003\n",
      "Iteration 645, loss = 0.07539296\n",
      "Iteration 646, loss = 0.07515647\n",
      "Iteration 647, loss = 0.07469899\n",
      "Iteration 648, loss = 0.07573655\n",
      "Iteration 649, loss = 0.07460574\n",
      "Iteration 650, loss = 0.07498933\n",
      "Iteration 651, loss = 0.07373579\n",
      "Iteration 652, loss = 0.07380671\n",
      "Iteration 653, loss = 0.07371603\n",
      "Iteration 654, loss = 0.07368794\n",
      "Iteration 655, loss = 0.07355661\n",
      "Iteration 656, loss = 0.07384655\n",
      "Iteration 657, loss = 0.07187290\n",
      "Iteration 658, loss = 0.07200786\n",
      "Iteration 659, loss = 0.07176884\n",
      "Iteration 660, loss = 0.07260658\n",
      "Iteration 661, loss = 0.07165845\n",
      "Iteration 662, loss = 0.07312496\n",
      "Iteration 663, loss = 0.07241578\n",
      "Iteration 664, loss = 0.07056415\n",
      "Iteration 665, loss = 0.07143467\n",
      "Iteration 666, loss = 0.07128492\n",
      "Iteration 667, loss = 0.07166430\n",
      "Iteration 668, loss = 0.07088071\n",
      "Iteration 669, loss = 0.07154371\n",
      "Iteration 670, loss = 0.07352291\n",
      "Iteration 671, loss = 0.07026014\n",
      "Iteration 672, loss = 0.06969995\n",
      "Iteration 673, loss = 0.06880004\n",
      "Iteration 674, loss = 0.06849444\n",
      "Iteration 675, loss = 0.06849111\n",
      "Iteration 676, loss = 0.06879066\n",
      "Iteration 677, loss = 0.06981204\n",
      "Iteration 678, loss = 0.06803040\n",
      "Iteration 679, loss = 0.06829346\n",
      "Iteration 680, loss = 0.06913932\n",
      "Iteration 681, loss = 0.06750240\n",
      "Iteration 682, loss = 0.06763508\n",
      "Iteration 683, loss = 0.06713199\n",
      "Iteration 684, loss = 0.06720710\n",
      "Iteration 685, loss = 0.06663328\n",
      "Iteration 686, loss = 0.06719200\n",
      "Iteration 687, loss = 0.06698768\n",
      "Iteration 688, loss = 0.06681963\n",
      "Iteration 689, loss = 0.06773815\n",
      "Iteration 690, loss = 0.06594982\n",
      "Iteration 691, loss = 0.06592438\n",
      "Iteration 692, loss = 0.06657931\n",
      "Iteration 693, loss = 0.06575474\n",
      "Iteration 694, loss = 0.06524929\n",
      "Iteration 695, loss = 0.06552051\n",
      "Iteration 696, loss = 0.06488200\n",
      "Iteration 697, loss = 0.06516756\n",
      "Iteration 698, loss = 0.06511533\n",
      "Iteration 699, loss = 0.06513104\n",
      "Iteration 700, loss = 0.06538061\n",
      "Iteration 701, loss = 0.06466312\n",
      "Iteration 702, loss = 0.06499850\n",
      "Iteration 703, loss = 0.06493811\n",
      "Iteration 704, loss = 0.06419291\n",
      "Iteration 705, loss = 0.06359289\n",
      "Iteration 706, loss = 0.06485137\n",
      "Iteration 707, loss = 0.06429970\n",
      "Iteration 708, loss = 0.06261101\n",
      "Iteration 709, loss = 0.06392008\n",
      "Iteration 710, loss = 0.06260214\n",
      "Iteration 711, loss = 0.06289883\n",
      "Iteration 712, loss = 0.06281117\n",
      "Iteration 713, loss = 0.06405073\n",
      "Iteration 714, loss = 0.06334551\n",
      "Iteration 715, loss = 0.06270882\n",
      "Iteration 716, loss = 0.06305210\n",
      "Iteration 717, loss = 0.06265128\n",
      "Iteration 718, loss = 0.06127446\n",
      "Iteration 719, loss = 0.06175866\n",
      "Iteration 720, loss = 0.06107857\n",
      "Iteration 721, loss = 0.06158818\n",
      "Iteration 722, loss = 0.06048165\n",
      "Iteration 723, loss = 0.06087750\n",
      "Iteration 724, loss = 0.06093752\n",
      "Iteration 725, loss = 0.06202199\n",
      "Iteration 726, loss = 0.06302948\n",
      "Iteration 727, loss = 0.06141590\n",
      "Iteration 728, loss = 0.06045772\n",
      "Iteration 729, loss = 0.05912223\n",
      "Iteration 730, loss = 0.06001329\n",
      "Iteration 731, loss = 0.05896174\n",
      "Iteration 732, loss = 0.05873502\n",
      "Iteration 733, loss = 0.05859975\n",
      "Iteration 734, loss = 0.05891105\n",
      "Iteration 735, loss = 0.05855244\n",
      "Iteration 736, loss = 0.05904980\n",
      "Iteration 737, loss = 0.05824927\n",
      "Iteration 738, loss = 0.05947697\n",
      "Iteration 739, loss = 0.05861210\n",
      "Iteration 740, loss = 0.05811657\n",
      "Iteration 741, loss = 0.05767596\n",
      "Iteration 742, loss = 0.05802623\n",
      "Iteration 743, loss = 0.05769059\n",
      "Iteration 744, loss = 0.05881117\n",
      "Iteration 745, loss = 0.05905904\n",
      "Iteration 746, loss = 0.05731224\n",
      "Iteration 747, loss = 0.05681027\n",
      "Iteration 748, loss = 0.05842714\n",
      "Iteration 749, loss = 0.05725246\n",
      "Iteration 750, loss = 0.05700681\n",
      "Iteration 751, loss = 0.05642650\n",
      "Iteration 752, loss = 0.05653378\n",
      "Iteration 753, loss = 0.05588346\n",
      "Iteration 754, loss = 0.05622962\n",
      "Iteration 755, loss = 0.05631618\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 756, loss = 0.05507405\n",
      "Iteration 757, loss = 0.05534947\n",
      "Iteration 758, loss = 0.05640424\n",
      "Iteration 759, loss = 0.05584593\n",
      "Iteration 760, loss = 0.05480038\n",
      "Iteration 761, loss = 0.05546167\n",
      "Iteration 762, loss = 0.05437824\n",
      "Iteration 763, loss = 0.05446890\n",
      "Iteration 764, loss = 0.05433228\n",
      "Iteration 765, loss = 0.05469914\n",
      "Iteration 766, loss = 0.05425016\n",
      "Iteration 767, loss = 0.05439925\n",
      "Iteration 768, loss = 0.05384726\n",
      "Iteration 769, loss = 0.05391771\n",
      "Iteration 770, loss = 0.05357155\n",
      "Iteration 771, loss = 0.05655946\n",
      "Iteration 772, loss = 0.05497951\n",
      "Iteration 773, loss = 0.05361173\n",
      "Iteration 774, loss = 0.05335212\n",
      "Iteration 775, loss = 0.05274916\n",
      "Iteration 776, loss = 0.05379792\n",
      "Iteration 777, loss = 0.05429496\n",
      "Iteration 778, loss = 0.05356791\n",
      "Iteration 779, loss = 0.05260102\n",
      "Iteration 780, loss = 0.05246071\n",
      "Iteration 781, loss = 0.05251492\n",
      "Iteration 782, loss = 0.05184763\n",
      "Iteration 783, loss = 0.05149131\n",
      "Iteration 784, loss = 0.05170052\n",
      "Iteration 785, loss = 0.05199334\n",
      "Iteration 786, loss = 0.05217426\n",
      "Iteration 787, loss = 0.05194311\n",
      "Iteration 788, loss = 0.05176272\n",
      "Iteration 789, loss = 0.05122456\n",
      "Iteration 790, loss = 0.05092292\n",
      "Iteration 791, loss = 0.05245672\n",
      "Iteration 792, loss = 0.05279772\n",
      "Iteration 793, loss = 0.05139444\n",
      "Iteration 794, loss = 0.05234871\n",
      "Iteration 795, loss = 0.05070869\n",
      "Iteration 796, loss = 0.05093792\n",
      "Iteration 797, loss = 0.04975195\n",
      "Iteration 798, loss = 0.05088840\n",
      "Iteration 799, loss = 0.05258690\n",
      "Iteration 800, loss = 0.05155704\n",
      "Iteration 801, loss = 0.05033101\n",
      "Iteration 802, loss = 0.05044967\n",
      "Iteration 803, loss = 0.05051746\n",
      "Iteration 804, loss = 0.04952010\n",
      "Iteration 805, loss = 0.04858137\n",
      "Iteration 806, loss = 0.04893905\n",
      "Iteration 807, loss = 0.04973534\n",
      "Iteration 808, loss = 0.04852609\n",
      "Iteration 809, loss = 0.04934596\n",
      "Iteration 810, loss = 0.04893789\n",
      "Iteration 811, loss = 0.04887234\n",
      "Iteration 812, loss = 0.04870535\n",
      "Iteration 813, loss = 0.04855251\n",
      "Iteration 814, loss = 0.04829719\n",
      "Iteration 815, loss = 0.04832738\n",
      "Iteration 816, loss = 0.04839039\n",
      "Iteration 817, loss = 0.04793833\n",
      "Iteration 818, loss = 0.04943632\n",
      "Iteration 819, loss = 0.04820839\n",
      "Iteration 820, loss = 0.05006666\n",
      "Iteration 821, loss = 0.04723678\n",
      "Iteration 822, loss = 0.04775813\n",
      "Iteration 823, loss = 0.04761867\n",
      "Iteration 824, loss = 0.04748944\n",
      "Iteration 825, loss = 0.04731898\n",
      "Iteration 826, loss = 0.04697327\n",
      "Iteration 827, loss = 0.04686381\n",
      "Iteration 828, loss = 0.04580610\n",
      "Iteration 829, loss = 0.04709762\n",
      "Iteration 830, loss = 0.04855237\n",
      "Iteration 831, loss = 0.04753137\n",
      "Iteration 832, loss = 0.04634460\n",
      "Iteration 833, loss = 0.04678428\n",
      "Iteration 834, loss = 0.04584558\n",
      "Iteration 835, loss = 0.04568742\n",
      "Iteration 836, loss = 0.04574905\n",
      "Iteration 837, loss = 0.04572763\n",
      "Iteration 838, loss = 0.04718206\n",
      "Iteration 839, loss = 0.04522325\n",
      "Iteration 840, loss = 0.04541595\n",
      "Iteration 841, loss = 0.04606436\n",
      "Iteration 842, loss = 0.04587686\n",
      "Iteration 843, loss = 0.04490723\n",
      "Iteration 844, loss = 0.04566903\n",
      "Iteration 845, loss = 0.04557464\n",
      "Iteration 846, loss = 0.04440316\n",
      "Iteration 847, loss = 0.04452831\n",
      "Iteration 848, loss = 0.04501606\n",
      "Iteration 849, loss = 0.04474052\n",
      "Iteration 850, loss = 0.04463381\n",
      "Iteration 851, loss = 0.04497355\n",
      "Iteration 852, loss = 0.04390186\n",
      "Iteration 853, loss = 0.04334396\n",
      "Iteration 854, loss = 0.04417254\n",
      "Iteration 855, loss = 0.04414512\n",
      "Iteration 856, loss = 0.04406866\n",
      "Iteration 857, loss = 0.04335832\n",
      "Iteration 858, loss = 0.04307406\n",
      "Iteration 859, loss = 0.04327835\n",
      "Iteration 860, loss = 0.04345676\n",
      "Iteration 861, loss = 0.04288056\n",
      "Iteration 862, loss = 0.04328181\n",
      "Iteration 863, loss = 0.04363062\n",
      "Iteration 864, loss = 0.04392124\n",
      "Iteration 865, loss = 0.04273980\n",
      "Iteration 866, loss = 0.04240708\n",
      "Iteration 867, loss = 0.04238491\n",
      "Iteration 868, loss = 0.04214670\n",
      "Iteration 869, loss = 0.04259010\n",
      "Iteration 870, loss = 0.04230143\n",
      "Iteration 871, loss = 0.04219799\n",
      "Iteration 872, loss = 0.04426582\n",
      "Iteration 873, loss = 0.04330245\n",
      "Iteration 874, loss = 0.04204713\n",
      "Iteration 875, loss = 0.04223792\n",
      "Iteration 876, loss = 0.04191535\n",
      "Iteration 877, loss = 0.04134575\n",
      "Iteration 878, loss = 0.04070144\n",
      "Iteration 879, loss = 0.04098617\n",
      "Iteration 880, loss = 0.04182162\n",
      "Iteration 881, loss = 0.04185280\n",
      "Iteration 882, loss = 0.04120915\n",
      "Iteration 883, loss = 0.04072551\n",
      "Iteration 884, loss = 0.04089632\n",
      "Iteration 885, loss = 0.03999037\n",
      "Iteration 886, loss = 0.04064151\n",
      "Iteration 887, loss = 0.04055793\n",
      "Iteration 888, loss = 0.04129911\n",
      "Iteration 889, loss = 0.04141340\n",
      "Iteration 890, loss = 0.04133977\n",
      "Iteration 891, loss = 0.04143559\n",
      "Iteration 892, loss = 0.04147283\n",
      "Iteration 893, loss = 0.03955028\n",
      "Iteration 894, loss = 0.04074043\n",
      "Iteration 895, loss = 0.04022032\n",
      "Iteration 896, loss = 0.04231106\n",
      "Iteration 897, loss = 0.04033044\n",
      "Iteration 898, loss = 0.03958805\n",
      "Iteration 899, loss = 0.03999641\n",
      "Iteration 900, loss = 0.03963967\n",
      "Iteration 901, loss = 0.03888782\n",
      "Iteration 902, loss = 0.03967836\n",
      "Iteration 903, loss = 0.03956098\n",
      "Iteration 904, loss = 0.03924009\n",
      "Iteration 905, loss = 0.03841880\n",
      "Iteration 906, loss = 0.03864536\n",
      "Iteration 907, loss = 0.03879112\n",
      "Iteration 908, loss = 0.04019811\n",
      "Iteration 909, loss = 0.03924276\n",
      "Iteration 910, loss = 0.03888143\n",
      "Iteration 911, loss = 0.03844841\n",
      "Iteration 912, loss = 0.03951524\n",
      "Iteration 913, loss = 0.03854913\n",
      "Iteration 914, loss = 0.03894594\n",
      "Iteration 915, loss = 0.03814273\n",
      "Iteration 916, loss = 0.03850929\n",
      "Iteration 917, loss = 0.03890588\n",
      "Iteration 918, loss = 0.03967958\n",
      "Iteration 919, loss = 0.03908820\n",
      "Iteration 920, loss = 0.03816173\n",
      "Iteration 921, loss = 0.04000811\n",
      "Iteration 922, loss = 0.03731036\n",
      "Iteration 923, loss = 0.03812146\n",
      "Iteration 924, loss = 0.03823400\n",
      "Iteration 925, loss = 0.03900445\n",
      "Iteration 926, loss = 0.03806715\n",
      "Iteration 927, loss = 0.03853698\n",
      "Iteration 928, loss = 0.03908736\n",
      "Iteration 929, loss = 0.03822905\n",
      "Iteration 930, loss = 0.03735722\n",
      "Iteration 931, loss = 0.03724788\n",
      "Iteration 932, loss = 0.03892160\n",
      "Iteration 933, loss = 0.03816475\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-5 {color: black;}#sk-container-id-5 pre{padding: 0;}#sk-container-id-5 div.sk-toggleable {background-color: white;}#sk-container-id-5 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-5 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-5 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-5 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-5 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-5 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-5 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-5 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-5 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-5 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-5 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-5 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-5 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-5 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-5 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-5 div.sk-item {position: relative;z-index: 1;}#sk-container-id-5 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-5 div.sk-item::before, #sk-container-id-5 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-5 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-5 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-5 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-5 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-5 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-5 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-5 div.sk-label-container {text-align: center;}#sk-container-id-5 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-5 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-5\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(max_iter=1000, verbose=True)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" checked><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(max_iter=1000, verbose=True)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(max_iter=1000, verbose=True)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Default settings create 1 hidden layer with 100 neurons\n",
    "mlp_clf = MLPClassifier(hidden_layer_sizes=(100,), max_iter=1000, verbose=True)\n",
    "\n",
    "mlp_clf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9955787781350482"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict the train values\n",
    "train_y_pred = mlp_clf.predict(train_x)\n",
    "\n",
    "#Train accuracy\n",
    "accuracy_score(train_y, train_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.598875351452671"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict the test values\n",
    "test_y_pred = mlp_clf.predict(test_x)\n",
    "\n",
    "#Test accuracy\n",
    "accuracy_score(test_y, test_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Change the number of neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mural\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(50,), max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(50,), max_iter=1000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(50,), max_iter=1000)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Increase neurons from 100 to 50\n",
    "mlp_clf = MLPClassifier(max_iter=1000, verbose=False,\n",
    "                        hidden_layer_sizes=(50,))\n",
    "\n",
    "mlp_clf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9533762057877814"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict the train values\n",
    "train_y_pred = mlp_clf.predict(train_x)\n",
    "\n",
    "#Train accuracy\n",
    "accuracy_score(train_y, train_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6119962511715089"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict the test values\n",
    "test_y_pred = mlp_clf.predict(test_x)\n",
    "\n",
    "#Test accuracy\n",
    "accuracy_score(test_y, test_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-8 {color: black;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(50, 25, 10), max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(50, 25, 10), max_iter=1000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(50, 25, 10), max_iter=1000)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_clf = MLPClassifier(hidden_layer_sizes=(50,25,10),\n",
    "                       max_iter=1000)\n",
    "\n",
    "dnn_clf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "287"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's check the number of iterations:\n",
    "dnn_clf.n_iter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's check the number of layers: we gave 3 but get 5 coz i/p and o/p included\n",
    "dnn_clf.n_layers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9714630225080386"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict the train values\n",
    "train_y_pred = dnn_clf.predict(train_x)\n",
    "\n",
    "#Train accuracy\n",
    "accuracy_score(train_y, train_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5857544517338332"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict the test values\n",
    "test_y_pred = dnn_clf.predict(test_x)\n",
    "\n",
    "#Test accuracy\n",
    "accuracy_score(test_y, test_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deeper Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-9 {color: black;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(100, 80, 60, 40, 20, 10), max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" checked><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(100, 80, 60, 40, 20, 10), max_iter=1000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(100, 80, 60, 40, 20, 10), max_iter=1000)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_clf = MLPClassifier(hidden_layer_sizes=(100,80,60,40,20,10),\n",
    "                       max_iter=1000)\n",
    "\n",
    "dnn_clf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's check the number of iterations:\n",
    "dnn_clf.n_iter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's check the number of layers:\n",
    "dnn_clf.n_layers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.987540192926045"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict the train values\n",
    "train_y_pred = dnn_clf.predict(train_x)\n",
    "\n",
    "#Train accuracy\n",
    "accuracy_score(train_y, train_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6204311152764761"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict the test values\n",
    "test_y_pred = dnn_clf.predict(test_x)\n",
    "\n",
    "#Test accuracy\n",
    "accuracy_score(test_y, test_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-11 {color: black;}#sk-container-id-11 pre{padding: 0;}#sk-container-id-11 div.sk-toggleable {background-color: white;}#sk-container-id-11 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-11 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-11 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-11 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-11 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-11 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-11 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-11 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-11 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-11 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-11 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-11 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-11 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-11 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-11 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-11 div.sk-item {position: relative;z-index: 1;}#sk-container-id-11 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-11 div.sk-item::before, #sk-container-id-11 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-11 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-11 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-11 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-11 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-11 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-11 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-11 div.sk-label-container {text-align: center;}#sk-container-id-11 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-11 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-11\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(early_stopping=True, hidden_layer_sizes=(50, 25, 10),\n",
       "              max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-11\" type=\"checkbox\" checked><label for=\"sk-estimator-id-11\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(early_stopping=True, hidden_layer_sizes=(50, 25, 10),\n",
       "              max_iter=1000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(early_stopping=True, hidden_layer_sizes=(50, 25, 10),\n",
       "              max_iter=1000)"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_clf = MLPClassifier(hidden_layer_sizes=(50,25,10),\n",
    "                       max_iter=1000,\n",
    "                       early_stopping=True)\n",
    "\n",
    "dnn_clf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's check the number of iterations:\n",
    "dnn_clf.n_iter_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6495176848874598"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict the train values\n",
    "train_y_pred = dnn_clf.predict(train_x)\n",
    "\n",
    "#Train accuracy\n",
    "accuracy_score(train_y, train_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6457357075913777"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict the test values\n",
    "test_y_pred = dnn_clf.predict(test_x)\n",
    "\n",
    "#Test accuracy\n",
    "accuracy_score(test_y, test_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-13 {color: black;}#sk-container-id-13 pre{padding: 0;}#sk-container-id-13 div.sk-toggleable {background-color: white;}#sk-container-id-13 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-13 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-13 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-13 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-13 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-13 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-13 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-13 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-13 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-13 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-13 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-13 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-13 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-13 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-13 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-13 div.sk-item {position: relative;z-index: 1;}#sk-container-id-13 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-13 div.sk-item::before, #sk-container-id-13 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-13 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-13 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-13 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-13 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-13 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-13 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-13 div.sk-label-container {text-align: center;}#sk-container-id-13 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-13 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-13\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(hidden_layer_sizes=(50, 25, 10), max_iter=1000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-13\" type=\"checkbox\" checked><label for=\"sk-estimator-id-13\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(hidden_layer_sizes=(50, 25, 10), max_iter=1000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(hidden_layer_sizes=(50, 25, 10), max_iter=1000)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_clf = MLPClassifier(hidden_layer_sizes=(50,25,10),\n",
    "                       max_iter=1000,\n",
    "                       activation = 'relu')\n",
    "\n",
    "dnn_clf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9903536977491961"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict the train values\n",
    "train_y_pred = dnn_clf.predict(train_x)\n",
    "\n",
    "#Train accuracy\n",
    "accuracy_score(train_y, train_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5913776944704779"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict the test values\n",
    "test_y_pred = dnn_clf.predict(test_x)\n",
    "\n",
    "#Test accuracy\n",
    "accuracy_score(test_y, test_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solver (Optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mural\\anaconda3\\Lib\\site-packages\\sklearn\\neural_network\\_multilayer_perceptron.py:691: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-15 {color: black;}#sk-container-id-15 pre{padding: 0;}#sk-container-id-15 div.sk-toggleable {background-color: white;}#sk-container-id-15 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-15 label.sk-toggleable__label-arrow:before {content: \"â–¸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-15 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-15 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-15 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-15 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-15 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-15 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"â–¾\";}#sk-container-id-15 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-15 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-15 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-15 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-15 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-15 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-15 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-15 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-15 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-15 div.sk-item {position: relative;z-index: 1;}#sk-container-id-15 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-15 div.sk-item::before, #sk-container-id-15 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-15 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-15 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-15 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-15 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-15 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-15 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-15 div.sk-label-container {text-align: center;}#sk-container-id-15 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-15 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-15\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>MLPClassifier(activation=&#x27;tanh&#x27;, hidden_layer_sizes=(50, 25, 10), max_iter=1000,\n",
       "              solver=&#x27;sgd&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-15\" type=\"checkbox\" checked><label for=\"sk-estimator-id-15\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">MLPClassifier</label><div class=\"sk-toggleable__content\"><pre>MLPClassifier(activation=&#x27;tanh&#x27;, hidden_layer_sizes=(50, 25, 10), max_iter=1000,\n",
       "              solver=&#x27;sgd&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "MLPClassifier(activation='tanh', hidden_layer_sizes=(50, 25, 10), max_iter=1000,\n",
       "              solver='sgd')"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's use Stochastic Gradient Descent optimizer\n",
    "\n",
    "dnn_clf = MLPClassifier(hidden_layer_sizes=(50,25,10),\n",
    "                       max_iter=1000,\n",
    "                       activation = 'tanh',\n",
    "                       solver='sgd')\n",
    "\n",
    "dnn_clf.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7508038585209004"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict the train values\n",
    "train_y_pred = dnn_clf.predict(train_x)\n",
    "\n",
    "#Train accuracy\n",
    "accuracy_score(train_y, train_y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6401124648547329"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict the test values\n",
    "test_y_pred = dnn_clf.predict(test_x)\n",
    "\n",
    "#Test accuracy\n",
    "accuracy_score(test_y, test_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.85449093, -0.64236262, -0.02173662, -0.43231232, -0.32724017,\n",
       "         0.42133671,  1.25362268, -0.40315781,  1.26658906, -0.14002161,\n",
       "         2.9856231 ,  1.12556644,  0.55844268,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         1.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  1.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  1.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  1.        ,\n",
       "         0.        ,  0.        ,  1.        ,  0.        ,  1.        ,\n",
       "         1.        ,  0.        ,  1.        ,  0.        ]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Select a random observation\n",
    "\n",
    "random = test_x[50:51]\n",
    "random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.70995983, 0.25998934, 0.0140241 , 0.01602673]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Observe the input variables of the observation\n",
    "dnn_clf.predict_proba(random)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.71, 0.26, 0.01, 0.02]])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Round the probability values\n",
    "np.round(dnn_clf.predict_proba(random), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['btw_$151-$225', 'btw_$75-$150', 'gte_226', 'lte_$75'],\n",
       "      dtype='<U13')"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dnn_clf.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "196    btw_$75-$150\n",
       "Name: price_category, dtype: object"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_y[50:51]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "nav_menu": {
   "height": "279px",
   "width": "309px"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
